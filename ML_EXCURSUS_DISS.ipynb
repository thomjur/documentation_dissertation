{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Excursus\n",
    "## (Documentation of my Dissertation \"The Notion of *surb* in Ancient Armenian Texts from the Fifth Century CE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes several performance tests for different ML classifiers of the annotated Twitter Data from the dissertation \"The Notion of *surb* in the Ancient Armenian Church\" (Thomas Jurczyk). The data itself is not part of the GitHub repository. This file only serves as a documentation of the code used to create and test the different ML classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# importing classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# functions for the evaluation of the ML models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "# other modules\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import linspace\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Loading and Preparing Annotated Twitter Data for the ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_annotated = pd.read_csv(\"annotated_data/full_data_revised.csv\", sep=\";\")\n",
    "# reducing frame to annotated data\n",
    "twitter_annotated = twitter_annotated.iloc[:3250]\n",
    "twitter_annotated[\"RON_revised\"] = twitter_annotated[\"RON_revised\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(twitter_annotated.shape) + \"\\n\")\n",
    "twitter_annotated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the (vectorized) frames for the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(twitter_annotated[\"Text\"])\n",
    "y = twitter_annotated[\"RON_revised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing approximately 25% of the frame in a later test set X_test, y_test; the rest is stored in X_train, y_train and used for the training of the ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X[:2438], y.iloc[:2438], X[2438:], y.iloc[2438:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Training and Testing of the Selected ML Models\n",
    "The evaluation only includes evaluations of recall, precision, and the F<sub>1</sub> score for each model. There is neither a PR/TR curve nor a ROC curve to optimize a threshold of certain models, such as the SGDClassifier. Firstly, because optimization of thresholds goes beyond the simple use of ML in this thesis. Secondly, instance-based models such as KNN have no thresholds but would need a variation of the k-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printScores(y_orig, y_predict):\n",
    "    '''\n",
    "        helper function for evaluateModels()\n",
    "    '''\n",
    "    print(\"Precision Score: \" + str(precision_score(y_orig, y_predict)) + \"\\n\")\n",
    "    print(\"Recall: \" + str(recall_score(y_orig, y_predict)) + \"\\n\")\n",
    "    print(\"F_1 Score: \"  + str(f1_score(y_orig, y_predict)) + \"\\n\")\n",
    "    print(\"Confusion matrix:\\n\"  + str(confusion_matrix(y_orig, y_predict)) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModelsCV(model_list, train_data, label_data):\n",
    "    best_model = {\"best_precision\": [\"model\", 0], \"best_recall\": [\"model\", 0], \"best_f1\": [\"model\", 0]}\n",
    "    for model in model_list:\n",
    "        y_train_predict = cross_val_predict(model, train_data, label_data, cv=3)\n",
    "        precision = precision_score(label_data, y_train_predict)\n",
    "        recall = recall_score(label_data, y_train_predict)\n",
    "        f1_ = f1_score(label_data, y_train_predict)\n",
    "        # if scores are better than scores in the best_models dict, add model + value to the corresponding category in the dict()\n",
    "        if precision > best_model[\"best_precision\"][1]:\n",
    "            best_model[\"best_precision\"][0] = str(model)\n",
    "            best_model[\"best_precision\"][1] = precision\n",
    "        if recall > best_model[\"best_recall\"][1]:\n",
    "            best_model[\"best_recall\"][0] = str(model)\n",
    "            best_model[\"best_recall\"][1] = recall\n",
    "        if f1_ > best_model[\"best_f1\"][1]:\n",
    "            best_model[\"best_f1\"][0] = str(model)\n",
    "            best_model[\"best_f1\"][1] = f1_\n",
    "        print(\"Model: \" + str(model) + \"\\n\")\n",
    "        printScores(label_data, y_train_predict)\n",
    "    return best_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising classifiers\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "knn_clf = KNeighborsClassifier()\n",
    "forest_clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "svc_clf = SVC()\n",
    "logReg_clf = LogisticRegression()\n",
    "\n",
    "# creating list for function\n",
    "ml_models_list = [sgd_clf, knn_clf, forest_clf, svc_clf, logReg_clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using function and creating dict with best models for precision, recall, and f1_score\n",
    "dict_best_models = evaluateModelsCV(ml_models_list, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Final Test with the Test Data\n",
    "After the cross-validation in the last step, the final evaluation with the help of the test set is conducted in the following part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModelsFinal(model_list, train_data, train_label, test_data, test_label):\n",
    "    '''\n",
    "        evaluating test set with different trained ML classifiers; returning dict with best performing models for precision, recall, f1 and an overall dict with all classifiers plus their stats\n",
    "    '''\n",
    "    best_model = {\"best_precision\": [\"model\", 0], \"best_recall\": [\"model\", 0], \"best_f1\": [\"model\", 0]}\n",
    "    overall_stats_dict = dict()\n",
    "    for model in model_list:\n",
    "        model_trained = model.fit(train_data, train_label)\n",
    "        y_test_predict = model_trained.predict(test_data) \n",
    "        precision = precision_score(test_label, y_test_predict)\n",
    "        recall = recall_score(test_label, y_test_predict)\n",
    "        f1_ = f1_score(test_label, y_test_predict)\n",
    "        # if scores are better than scores in the best_models dict, add model + value to the corresponding category in the dict()\n",
    "        if precision > best_model[\"best_precision\"][1]:\n",
    "            best_model[\"best_precision\"][0] = str(model)\n",
    "            best_model[\"best_precision\"][1] = precision\n",
    "        if recall > best_model[\"best_recall\"][1]:\n",
    "            best_model[\"best_recall\"][0] = str(model)\n",
    "            best_model[\"best_recall\"][1] = recall\n",
    "        if f1_ > best_model[\"best_f1\"][1]:\n",
    "            best_model[\"best_f1\"][0] = str(model)\n",
    "            best_model[\"best_f1\"][1] = f1_\n",
    "        print(\"Model: \" + str(model) + \"\\n\")\n",
    "        printScores(test_label, y_test_predict)\n",
    "        # adding values to the overall dict\n",
    "        classifier_name = str(model).split(\"(\")[0]\n",
    "        overall_stats_dict[classifier_name] = [precision, recall, f1_]\n",
    "    return (best_model, overall_stats_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_best_dict, final_classifiers_dict  = evaluateModelsFinal(ml_models_list, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', -1)\n",
    "plt.style.use('fivethirtyeight')\n",
    "colors = [cm.jet(x) for x in linspace(0.0,1.0,20)]\n",
    "colors2 = cm.tab20.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_x = list()\n",
    "f1_scores_y = list()\n",
    "for name, measures in final_classifiers_dict.items():\n",
    "    classifiers_x.append(name)\n",
    "    f1_scores_y.append(measures[2])\n",
    "#create df\n",
    "df_classifiers_f1_plot = pd.DataFrame()\n",
    "df_classifiers_f1_plot[\"Classifier\"] = classifiers_x\n",
    "df_classifiers_f1_plot[\"f1_scores\"] = f1_scores_y\n",
    "# dropping \"SVC\" due to 0 value; why it returns a zero should be looked into later\n",
    "df_classifiers_f1_plot.drop(3,inplace=True)\n",
    "df_classifiers_f1_plot.set_index(\"Classifier\", drop=True, inplace=True)\n",
    "df_classifiers_f1_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "bars = df_classifiers_f1_plot.plot(kind=\"bar\", legend=False, ax=plt.gca())\n",
    "for num, patch in enumerate(bars.patches):\n",
    "    patch.set_color(colors2[num])\n",
    "plt.tight_layout()\n",
    "plt.gca().set_xlabel(\"\")\n",
    "plt.savefig(\"current_fig.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
